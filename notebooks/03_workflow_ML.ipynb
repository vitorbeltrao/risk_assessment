{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import joblib\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, RocCurveDisplay, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train set as a pandas dataframe\n",
    "train_set = pd.read_csv('train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the features that we are going to use\n",
    "X = train_set.drop(['exited'], axis=1)\n",
    "y = train_set['exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. make pipelines to do the necessary transformations\n",
    "\n",
    "# 1.1 divide the qualitative and quantitative features\n",
    "quantitative_columns = selector(dtype_exclude=['object'])\n",
    "\n",
    "quantitative_columns = quantitative_columns(X)\n",
    "\n",
    "# 1.2 apply the respective transformations with columntransformer method\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), quantitative_columns)],\n",
    "     remainder='drop')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up MLFlow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/4YouSee/Desktop/personal_work/risk_assessment/notebooks/mlruns/145316482257306757', creation_time=1679691352994, experiment_id='145316482257306757', last_update_time=1679691352994, lifecycle_stage='active', name='04_risk_assessment', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Setting up the mlflow experiment\n",
    "# experiment_path = 'mlflow_experiments' # Defining the path of experiments in MLFlow\n",
    "# experiment_name = '04_risk_assessment' # Defining the experiment name in MLFlow\n",
    "\n",
    "# if(not(mlflow.get_experiment_by_name(experiment_name))): # If the experiment does not exist, create it\n",
    "#     mlflow.create_experiment(experiment_name)\n",
    "    \n",
    "# mlflow.set_experiment(experiment_name) # Set the current experiment to register in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting current date to save\n",
    "# year = str(datetime.today().year)\n",
    "# month = str(datetime.today().month)\n",
    "# if len(month) == 1:\n",
    "#     month = \"0\" + month\n",
    "# day = str(datetime.today().day)\n",
    "# if len(day) == 1:\n",
    "#     day = \"0\" + day\n",
    "# date = year + \"/\" + month + \"/\" + day "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier_models(X, y, cv, scoring):\n",
    "    '''Função que treina os seguintes modelos de machine learning:\n",
    "    RandomForestClassifier, DecisionTreeClassifier, SGDClassifier, SVC,\n",
    "    LGBMClassifier, GaussianNB.\n",
    "    A função aplica a validação cruzada no conjunto de dados e retorna a média\n",
    "    da métrica selecionada no conjunto de treino e validação.\n",
    "    As únicas métricas ativas são Acurácia e F1 score.\n",
    "    Os experimentos com os modelos são acompanhados pelo MLflow.\n",
    "    \n",
    "    :param X: (dataframe or numpy array) \n",
    "    Dataframe ou array com o conjunto de variáveis independentes.\n",
    "    \n",
    "    :param y: (series or numpy array)\n",
    "    Coluna ou array com a variável dependente.\n",
    "    \n",
    "    :param cv: (int)\n",
    "    Determina a estratégia de divisão de validação cruzada.\n",
    "    \n",
    "    :param scoring: (str)\n",
    "    Estratégia para avaliar o desempenho do modelo de validação cruzada no conjunto de validação.\n",
    "    Deve ser passada entre aspas ao chamar a função.\n",
    "    '''\n",
    "    # 1. Instantiate the models\n",
    "    lr = LogisticRegression()\n",
    "    rf = RandomForestClassifier()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    sgdc = SGDClassifier()\n",
    "    svc = SVC()\n",
    "    gnb = GaussianNB()\n",
    "    ada = AdaBoostClassifier()\n",
    "\n",
    "    # 2. train and evaluate the models\n",
    "    for model in (lr, rf, dt, sgdc, svc, gnb, ada):\n",
    "        pipe = Pipeline(\n",
    "            steps=[('preprocessor', preprocessor),\n",
    "                   ('classifier', model)\n",
    "                  ]\n",
    "                )\n",
    "        scores = cross_validate(pipe, X, y, return_train_score=True,\n",
    "                                scoring=scoring, cv=cv)\n",
    "\n",
    "        # train and validation with accuracy\n",
    "        if scoring == 'accuracy':\n",
    "            log_train_acc = np.mean(scores['train_score'])\n",
    "            log_test_acc = np.mean(scores['test_score'])\n",
    "\n",
    "            # track the experiment with accuraccy\n",
    "            mlflow.start_run(run_name = date) \n",
    "            mlflow.log_param('Date', date) \n",
    "            mlflow.log_param('Features', X.columns)\n",
    "            mlflow.log_param('Pre-processing', preprocessor) \n",
    "            mlflow.log_param('ML model', pipe[1])\n",
    "\n",
    "            mlflow.log_metric('Train_acc', log_train_acc)\n",
    "            mlflow.log_metric('Test_acc', log_test_acc)\n",
    "\n",
    "            mlflow.end_run()\n",
    "\n",
    "        # train and validation with f1\n",
    "        if scoring == 'f1':\n",
    "            log_train_f1 = np.mean(scores['train_score'])\n",
    "            log_test_f1 = np.mean(scores['test_score'])\n",
    "\n",
    "            # track the experiment with f1 score\n",
    "            mlflow.start_run(run_name = date) \n",
    "            mlflow.log_param('Date', date) \n",
    "            mlflow.log_param('Features', X.columns)\n",
    "            mlflow.log_param('Pre-processing', preprocessor) \n",
    "            mlflow.log_param('ML model', pipe[1])\n",
    "\n",
    "            mlflow.log_metric('Train_f1', log_train_f1)\n",
    "            mlflow.log_metric('Test_f1', log_test_f1)\n",
    "\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run classifiers\n",
    "# run_classifier_models(X, y, 5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters were: {'dt__max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tunning\n",
    "# 1. Instantiate the pipeline\n",
    "final_model = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor), \n",
    "        ('dt', DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Hyperparameter interval to be tested\n",
    "param_grid = {\n",
    "    'dt__max_depth': [None, 3, 5, 10, 15, 20, 50, 100],\n",
    "}\n",
    "\n",
    "# 3. Training and apply grid search with cross validation\n",
    "grid_search = GridSearchCV(final_model, param_grid, cv = 5, scoring = 'accuracy',\n",
    "                           return_train_score = True)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Seeing the best hyperparameters for the model\n",
    "print('The best hyperparameters were:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean test score and mean train score is, respectively: (0.72, 0.9233333333333332)\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "cvres = [(mean_test_score,\n",
    "            mean_train_score) for mean_test_score,\n",
    "            mean_train_score in sorted(zip(cvres['mean_test_score'],\n",
    "                                        cvres['mean_train_score']),\n",
    "                                    reverse=True) if (math.isnan(mean_test_score) != True)]\n",
    "print(\n",
    "    'The mean test score and mean train score is, respectively:',\n",
    "    cvres[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model in Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk_assessment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
